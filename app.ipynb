{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import QueryType, VectorizedQuery,QueryAnswerType, QueryCaptionType\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "wiki_index = os.getenv(\"AZURE_SEARCH_INDEX_WIKI\")\n",
    "video_index = os.getenv(\"AZURE_SEARCH_INDEX_VIDEO\")\n",
    "api_version = os.environ.get(\"AZURE_OPENAI_PREVIEW_API_VERSION\")\n",
    "aoai_api_key = os.environ.get(\"AZURE_OPENAI_KEY\")\n",
    "embedding_model = os.environ.get(\"AZURE_OPENAI_EMBEDDING_NAME\")\n",
    "chat_model = os.environ.get(\"AZURE_OPENAI_MODEL\")\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "ad_token_provider = get_bearer_token_provider(credential,\"https://cognitiveservices.azure.com/.default\")\n",
    "azure_endpoint = (\n",
    "            os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "            if os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "            else f\"https://{os.environ.get('AZURE_OPENAI_RESOURCE')}.openai.azure.com/\")\n",
    "default_headers = {\"x-ms-useragent\": \"GitHubSampleWebApp/AsyncAzureOpenAI/1.0.0\"}\n",
    "\n",
    "wiki_search_client = SearchClient(service_endpoint, wiki_index, credential)\n",
    "video_search_client = SearchClient(service_endpoint, video_index, credential)\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "  api_version = api_version, \n",
    "  api_key  = aoai_api_key,\n",
    "  azure_ad_token_provider = ad_token_provider,\n",
    "  azure_endpoint = azure_endpoint,\n",
    "  default_headers = default_headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(query, model):\n",
    "    # Generate embeddings for the provided text using the specified model\n",
    "    embeddings_response = openai_client.embeddings.create(model=model, input=query)\n",
    "    # Extract the embedding data from the response\n",
    "    embedding = embeddings_response.data[0].embedding\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how to get cad into your workspace\"\n",
    "# query = \"Registering a PDM Link Server\"\n",
    "vector_query = VectorizedQuery(vector=generate_embeddings(query, embedding_model), \n",
    "                               k_nearest_neighbors=3, fields=\"text_vector\",)\n",
    "\n",
    "results_wiki = list(wiki_search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"chunk\", \"url_metadata\"],\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name=\"semantic\",\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=3,\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_video = list(video_search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"chunk\", \"url_metadata\"],\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name=\"semantic\",\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=3,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_answers = results_wiki.get_answers()\n",
    "# for answer in semantic_answers:\n",
    "#     if answer.highlights:\n",
    "#         print(f\"Semantic Answer: {answer.highlights}\")\n",
    "#     else:\n",
    "#         print(f\"Semantic Answer: {answer.text}\")\n",
    "#     print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "# for result in results_wiki:\n",
    "#     print(f\"Title: {result['title']}\")\n",
    "#     print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "#     print(f\"URL: {result['url_metadata']}\")\n",
    "#     captions = result[\"@search.captions\"]\n",
    "#     if captions:\n",
    "#         caption = captions[0]\n",
    "#         if caption.highlights:\n",
    "#             print(f\"Caption: {caption.highlights}\\n\")\n",
    "#         else:\n",
    "#             print(f\"Caption: {caption.text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_wiki = list(wiki_search_client.search(search_text=None, vector_queries= [query_embbeding], semantic_configuration_name='semantic', top=3, query_type=QueryType.SEMANTIC))\n",
    "# results_video = list(video_search_client.search(search_text=None,vector_queries= [query_embbeding],semantic_configuration_name='semantic', top=3, query_type=QueryType.SEMANTIC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results_wiki + results_video\n",
    "filter_result = []\n",
    "for d in results:\n",
    "    if d in results_wiki:\n",
    "        d['container'] = 'wiki'\n",
    "    elif d in results_video:\n",
    "        d['container'] = 'video'\n",
    "\n",
    "sorted_data = sorted(results_wiki + results_video, key=lambda x: x[\"@search.reranker_score\"], reverse=True)\n",
    "selected_container = sorted_data[0]['container']\n",
    "selected_chunks = []\n",
    "for i in sorted_data:\n",
    "    if i['container'] == selected_container and len(selected_chunks)<2:\n",
    "        selected_chunks.append(i)\n",
    "\n",
    "\n",
    "context_str = f\"\"\"\n",
    "**documents: 1**\n",
    "\n",
    "{selected_chunks[0]['chunk']}\n",
    "\n",
    "\n",
    "**documents: 2**\n",
    "\n",
    "{selected_chunks[1]['chunk']}\n",
    "\"\"\"\n",
    "\n",
    "if selected_container == \"video\":\n",
    "    citation_prompt = \"Context is a transcript which has timestamp in the form HH:MM:SS in each line above texts. Once you provide the answer, include the all the citations based on each of the timestamp and documents number [timestap, document number] at the end which are used to generate the answer. citation format is [['00:11:00', 1], ['00:1:44', 2]]\"\n",
    "else:\n",
    "    citation_prompt = \"Context is provided in the form of multiple documents. Once you provide the answer, include the citation based on the documents number at the end. citation format is [1,2]\"\n",
    "\n",
    "RAG_SYSTEM_PROMPT = f\"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "INSTRUCTIONS:\n",
    "1. You are an assistant who helps users answer their queries.\n",
    "2. Answer the user's question from the above Context.\n",
    "3. Give answer in step by step format.\n",
    "4. Keep your answer solely on the information given in the Context above.\n",
    "5. {citation_prompt}\n",
    "6. Do not create or derive your own answer. If the answer is not directly available in the context, just reply stating, 'There is no answer available'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "\n",
    "class Asnwer_Cittion(BaseModel):\n",
    "    \"\"\" \n",
    "    Asnwer and Citations\n",
    "    \"\"\"\n",
    "    if selected_container == 'video':\n",
    "        citation: List[List] = Field(\n",
    "            description=\"citations\")\n",
    "    else:\n",
    "        citation: List[int] = Field(\n",
    "            description=\"citations\")\n",
    "        \n",
    "    answer: str = Field(description=\"Answer of usr's question, do not include citations in answer\")\n",
    "    \n",
    "\n",
    "\n",
    "instructor_client = instructor.from_openai(openai_client)\n",
    "\n",
    "# Extract structured data from natural language\n",
    "user_info = instructor_client.chat.completions.create(\n",
    "    model=chat_model,\n",
    "    response_model=Asnwer_Cittion,\n",
    "    messages=[{\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "              {\"role\": \"user\", \"content\": query}],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how to get cad into your workspace'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get CAD into your workspace, follow these steps:\n",
      "\n",
      "1. **Check Out and Add to Workspace**: Locate the part you want to work on and check it out. When you check out a part, it means you have control over it, and no other user can modify it at the same time. This ensures that multiple users aren't working on the same part simultaneously. After checking out, add it to your workspace. [00:30:25, 1] [00:31:09, 1]\n",
      "\n",
      "2. **Alternative Method for Read-Only Users**: For read-only users or if you don't need to modify the part, use the 'Open in Creole' method. In the browser view, find the part you want and click on the square icon next to its file name column that says 'Open in Creole'. This action will add the part directly to your workspace. [00:37:59, 2] [00:38:30, 2] [00:38:51, 2]\n"
     ]
    }
   ],
   "source": [
    "print(user_info.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['00:30:25', 1], ['00:31:09', 1], ['00:37:59', 2], ['00:38:30', 2], ['00:38:51', 2]]\n"
     ]
    }
   ],
   "source": [
    "print(user_info.citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url_metadata': 'https://microsoft.sharepoint.com/:w:/t/collearninganddevelopmentlnd/EVb_PRHztPBFlkz5OHdKSNcB0qdvUuqnyvoaFnmmjw7dFQ?e=qZ2Dav',\n",
       " 'title': 'PDM_Training_02-Connecting_to_Server.docx',\n",
       " 'chunk': 'PDM Link Training Document #02\\nConnecting to the PDM Link Server\\n\\nThese are the steps for a new user to access the PDM Link server for the first time:\\n\\n\\n1) Launch Pro/E\\n\\n\\n2) Select the drop down menu FILE + MANAGE SESSION + SERVER MANAGEMENT as shown below:\\n(In Wildfire 5 use TOOLS + SERVER MANAGER) \\n\\n\\nThis will open the “Server Manager” window.\\n\\n\\n3) Select the drop down menu SERVER + REGISTER NEW SERVER as shown below:\\n\\n\\n\\n\\n\\n4) This will open the “Register New Server” window. Enter a name for the server (this can be anything, but “PDM Link” would be appropriate). Enter the location ( http://ptcwindchill/Windchill ) as shown below:\\n\\n\\n\\nThen select the “CHECK” box.\\n\\n\\n5) Now, PDM Link (or whatever else you called it) should show up in the Server Manager window as shown below:\\n\\n\\n\\n\\n6) Double click on the PDM Link server to connect to it. You will be prompted to enter your PDM Link user name and password. The prompt will look like the picture below. NOTE: This prompt window usually shows up behind your Pro/E window, so is not visible on the screen. Pick it on your task bar to bring it to the foreground.\\n\\n\\n7) Once you enter your user name and password, Pro/E will show the following prompt:\\n\\n\\n\\nSelect the YES box.\\n\\n\\n8) Your Pro/E session is now connected to PDM Link, and any new parts or saves will be done in your default workspace.\\n\\n\\n\\n\\n\\n\\n\\n4\\nRev. X1\\nimage3.JPG\\n\\nimage4.JPG\\n\\nimage5.JPG\\n\\nimage6.JPG\\n\\nimage1.jpg\\n\\nimage2.jpg',\n",
       " '@search.score': 0.03253968432545662,\n",
       " '@search.reranker_score': 3.6109068393707275,\n",
       " '@search.highlights': None,\n",
       " '@search.captions': [<azure.search.documents._generated.models._models_py3.QueryCaptionResult at 0x1fcffe14c40>],\n",
       " 'container': 'wiki'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "\n",
      "**documents: 1**\n",
      "\n",
      "working on that part. I can expect changes to be checked back in, maybe by that user in the near future.\n",
      "00:30:21 Speaker 1\n",
      "So check out and add to workspaces.\n",
      "00:30:23 Speaker 1\n",
      "What this page?\n",
      "00:30:24 Speaker 1\n",
      "Is we talked about all those same.\n",
      "00:30:25 Speaker 1\n",
      "Details I'm going to say OK.\n",
      "00:30:30 Speaker 1\n",
      "And now my workspace has that same copy of the same part, but it has a new status icon.\n",
      "00:30:36 Speaker 6\n",
      "If I can mouse over it.\n",
      "00:30:38 Speaker 1\n",
      "It says checked out by you. That means my username has control of this object and no other user in wind chill right now could go check it out at the same time.\n",
      "00:30:49 Speaker 1\n",
      "That's how we throttle a ton of users across a bunch of different countries and time zones actively working in the same.\n",
      "00:30:56 Speaker 1\n",
      "CAD database at the same.\n",
      "00:30:58 Speaker 1\n",
      "Time. If you want to change a part and you have read write access to it, your first step should be take control of the part so somebody else can't do it while you're trying to modify the same part. So.\n",
      "00:31:09 Speaker 1\n",
      "Check that part out.\n",
      "00:31:10 Speaker 1\n",
      "This is what the checkout looks like.\n",
      "00:31:13 Speaker 1\n",
      "If I've lost anybody, let.\n",
      "00:31:14 Speaker 1\n",
      "Me know.\n",
      "00:31:15 Speaker 1\n",
      "The next thing I'm going to do is I'm going to throw this copy out of my workspace and show you another way to get CAD into your workspace. That might be a little bit simpler for a read only user.\n",
      "00:31:25 Speaker 1\n",
      "So take that same object. I'm going to throw it out of my workspace. Remember, it's checked out to me.\n",
      "00:31:32 Speaker 1\n",
      "In throwing that out of my workspace, it needs to cancel my checkout and it will. It will tell me I have a little warning here that says this is checked out by you. I'm going to cancel your checkout if.\n",
      "00:31:41 Speaker 1\n",
      "You throw it out of your workspace.\n",
      "00:31:43 Speaker 1\n",
      "That's that's what I want to do. I'll get another prompt that tells me the same thing.\n",
      "00:31:48 Speaker 1\n",
      "\n",
      "\n",
      "**documents: 2**\n",
      "\n",
      "going to go a little fast.\n",
      "00:37:52 Speaker 1\n",
      "I'm back to an empty workspace now. Let's go back to the PDM link training product in the browser view.\n",
      "00:37:59 Speaker 1\n",
      "Here's my search remembered, and this is the part that I want. I call this the easy button sometimes, but just to the left of the file name column there is this square within a square icon that says open in Creole. This is a shortcut that you get for every single piece of CAD. This column is the same for all the CAD you'll see. That's a shortcut to do that.\n",
      "00:38:20 Speaker 1\n",
      "Open in Creole for this part right here. So once you find the part, I'm going to click this now open in Creole with one click. This is in my workspace, and I'm looking at that same CAD.\n",
      "00:38:30 Speaker 1\n",
      "This is probably the only way that you will be using to open CAD from read only products, so those of you that mentioned you're on the DFX team or you're somewhere where you're not actively developing or modifying CAD that add to or that open and creo button will be how you open CAD and and you can see it's immediately up on your screen.\n",
      "00:38:51 Speaker 1\n",
      "You could spin it around and analyze it however you want, but that's the most common way to get CAD into your workspace, especially as a read only user.\n",
      "00:38:59 Speaker 1\n",
      "Have I lost you so far?\n",
      "00:39:01 Speaker 1\n",
      "If so, please ask the question. Otherwise, I really hope everybody's following along and just maybe healthfully bored. OK, so now that I have the cat in my workspace, well, how do I go about modifying it in some way? And then if I'm happy with that modification, check it back in for everybody else to see.\n",
      "00:39:21 Speaker 1\n",
      "So that requires read write access to the product. In this case I do have read write access to where this part is and so I can modify this part if I want.\n",
      "00:39:30 Speaker 1\n",
      "Now this is not checked out to me right now. Remember I said as soon as you start modifying the part.\n",
      "\n",
      "---------------------\n",
      "INSTRUCTIONS:\n",
      "1. You are an assistant who helps users answer their queries.\n",
      "2. Answer the user's question from the above Context.\n",
      "3. Give answer in step by step format.\n",
      "4. Keep your answer solely on the information given in the Context above.\n",
      "5. Context is a transcript which has timestamp in the form HH:MM:SS in each line above texts in the transcript. Once you provide the answer, include the all the citations based on each of the timestamp and documents number [timestap, document number] at the end which are used to generate the answer. citation format is [['00:11:00', 1], ['00:1:44', 2]]\n",
      "6. Do not create or derive your own answer. If the answer is not directly available in the context, just reply stating, 'There is no answer available'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(RAG_SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving subscriptions...\n",
      "Logged in to Azure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: your version of azd is out of date, you have 1.9.6 and the latest version is 1.10.1\n",
      "\n",
      "To update to the latest version, run:\n",
      "winget upgrade Microsoft.Azd\n"
     ]
    }
   ],
   "source": [
    "!azd auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from backend.auth.auth_utils import get_authenticated_user_details\n",
    "from backend.security.ms_defender_utils import get_msdefender_user_json\n",
    "from quart import request\n",
    "# from llama_index.llms.azure_openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = os.environ.get(\"AZURE_OPENAI_PREVIEW_API_VERSION\")\n",
    "aoai_api_key = os.environ.get(\"AZURE_OPENAI_KEY\")\n",
    "ad_token_provider = get_bearer_token_provider(DefaultAzureCredential(),\"https://cognitiveservices.azure.com/.default\")\n",
    "endpoint = (\n",
    "            os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "            if os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "            else f\"https://{os.environ.get('AZURE_OPENAI_RESOURCE')}.openai.azure.com/\"\n",
    "        )\n",
    "default_headers = {\"x-ms-useragent\": \"GitHubSampleWebApp/AsyncAzureOpenAI/1.0.0\"}\n",
    "\n",
    "\n",
    "authenticated_user_details = get_authenticated_user_details({})\n",
    "conversation_id = None     \n",
    "user_json = get_msdefender_user_json(authenticated_user_details, {}, conversation_id)\n",
    "\n",
    "\n",
    "# from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "# llm = AzureOpenAI(\n",
    "#     engine=os.environ.get(\"AZURE_OPENAI_MODEL\"),\n",
    "#     model=os.environ.get(\"AZURE_OPENAI_MODEL_NAME\"),\n",
    "#     api_key=aoai_api_key,\n",
    "#     azure_endpoint=endpoint,\n",
    "#     api_version=api_version,\n",
    "#     AZURE_AD_TOKEN_PROVIDER = ad_token_provider,\n",
    "#     use_azure_ad =True,\n",
    "#     kwargs={'default_headers': default_headers, \"user\": user_json},\n",
    "# )\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name = os.environ.get(\"AZURE_OPENAI_MODEL\"),\n",
    "    openai_api_version = api_version,\n",
    "    openai_api_key = aoai_api_key,\n",
    "    azure_ad_token_provider = ad_token_provider,\n",
    "    default_headers = default_headers,\n",
    "    azure_endpoint = endpoint,\n",
    "    include_response_headers = True,\n",
    "    streaming= True,\n",
    "    temperature= 0.15,\n",
    "    # user_json = user_json\n",
    "    # model_kwargs = {'': user_json}\n",
    "    # logprobs=True,\n",
    "    model_kwargs={\"stream_options\": {\"include_usage\": True}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Ahoy there, matey! Welcome aboard! What brings ye to these treacherous waters today? Lookin' for treasure, a tale, or perhaps a bit o' both? Arrr!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with colorful personality.\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Hello\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_80a1bad4c7'}, id='run-3f724057-5589-4b85-9fd8-311050c8c8b3-0', usage_metadata={'input_tokens': 24, 'output_tokens': 28, 'total_tokens': 52})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that helps user.\",\n",
    "    ),\n",
    "    (\"human\", \"How are you?\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "# apim_request_id = ai_msg.response_metadata['headers']['apim-request-id']\n",
    "ai_msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.utils import (\n",
    "    format_as_ndjson,\n",
    "    format_stream_response,\n",
    "    format_non_streaming_response,\n",
    "    convert_to_pf_format,\n",
    "    format_pf_non_streaming_response,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
