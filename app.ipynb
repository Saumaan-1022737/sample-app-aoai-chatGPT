{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.11.7-py3-none-any.whl (6.8 kB)\n",
      "Collecting llama-index-readers-file<0.3.0,>=0.2.0\n",
      "  Downloading llama_index_readers_file-0.2.1-py3-none-any.whl (38 kB)\n",
      "Collecting llama-index-agent-openai<0.4.0,>=0.3.1\n",
      "  Using cached llama_index_agent_openai-0.3.1-py3-none-any.whl (13 kB)\n",
      "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4\n",
      "  Downloading llama_index_embeddings_openai-0.2.4-py3-none-any.whl (6.1 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48\n",
      "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.0/1.2 MB 991.0 kB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.1/1.2 MB 2.0 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.3/1.2 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.4/1.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.5/1.2 MB 2.9 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 0.6/1.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 0.8/1.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 0.9/1.2 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.1/1.2 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 2.8 MB/s eta 0:00:00\n",
      "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0\n",
      "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
      "Collecting llama-index-cli<0.4.0,>=0.3.0\n",
      "  Downloading llama_index_cli-0.3.0-py3-none-any.whl (27 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0\n",
      "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.7\n",
      "  Using cached llama_index_core-0.11.7-py3-none-any.whl (1.6 MB)\n",
      "Collecting llama-index-llms-openai<0.3.0,>=0.2.3\n",
      "  Using cached llama_index_llms_openai-0.2.3-py3-none-any.whl (12 kB)\n",
      "Collecting nltk>3.8.1\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl (9.5 kB)\n",
      "Collecting llama-index-program-openai<0.3.0,>=0.2.0\n",
      "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (1.44.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (0.27.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (2024.6.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\v-samomin\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (1.6.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (1.16.0)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (2.0.29)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (1.2.14)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (2.8.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (6.0.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (10.4.0)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (1.26.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (3.9.2)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (0.6.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (8.2.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (4.66.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (0.7.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (3.3)\n",
      "Collecting llama-cloud>=0.0.11\n",
      "  Downloading llama_cloud-0.0.17-py3-none-any.whl (187 kB)\n",
      "     ---------------------------------------- 0.0/187.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 187.4/187.4 kB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.5.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.2.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Collecting llama-parse>=0.5.0\n",
      "  Downloading llama_parse-0.5.2-py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.4.16)\n",
      "Requirement already satisfied: click in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (2024.2.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (1.3.1)\n",
      "Requirement already satisfied: idna in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (3.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (0.14.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (0.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (1.9.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index) (2.20.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index) (0.6.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.7->llama-index) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.7->llama-index) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.7->llama-index) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\v-samomin\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.12.0,>=0.11.7->llama-index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.7->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.7->llama-index) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\v-samomin\\appdata\\roaming\\python\\python310\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\v-samomin\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.7->llama-index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\v-samomin\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Installing collected packages: striprtf, dirtyjson, nltk, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-llms-openai, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.8.1\n",
      "    Uninstalling nltk-3.8.1:\n",
      "      Successfully uninstalled nltk-3.8.1\n",
      "Successfully installed dirtyjson-1.0.8 llama-cloud-0.0.17 llama-index-0.11.7 llama-index-agent-openai-0.3.1 llama-index-cli-0.3.0 llama-index-core-0.11.7 llama-index-embeddings-openai-0.2.4 llama-index-indices-managed-llama-cloud-0.3.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.3 llama-index-multi-modal-llms-openai-0.2.0 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.1 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.2 nltk-3.9.1 striprtf-0.0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-azure-openai\n",
      "  Using cached llama_index_llms_azure_openai-0.2.1-py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: httpx in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-llms-azure-openai) (0.27.0)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-llms-azure-openai) (0.2.3)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-llms-azure-openai) (1.15.0)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-llms-azure-openai) (0.11.7)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.30.2)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (43.0.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.2.0)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.24.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.30.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (3.9.1)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (3.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (0.7.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (1.2.14)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (6.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (4.66.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\v-samomin\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (1.6.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (8.2.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (10.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (4.11.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (2.8.2)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (1.0.8)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (3.9.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (2024.6.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (0.6.4)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (0.9.0)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (1.26.4)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-llms-openai<0.3.0,>=0.2.1->llama-index-llms-azure-openai) (0.3.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-index-llms-openai<0.3.0,>=0.2.1->llama-index-llms-azure-openai) (1.44.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (1.0.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (4.3.0)\n",
      "Requirement already satisfied: idna in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-llms-azure-openai) (0.14.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (1.3.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\v-samomin\\appdata\\roaming\\python\\python310\\site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from msal<2.0.0,>=1.24.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.9.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.10.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (2024.4.16)\n",
      "Requirement already satisfied: click in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (8.1.7)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.1->llama-index-llms-azure-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.1->llama-index-llms-azure-openai) (0.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\v-samomin\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx->llama-index-llms-azure-openai) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (2.20.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\v-samomin\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (3.21.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.22)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\v-samomin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-azure-openai) (23.2)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\v-samomin\\appdata\\roaming\\python\\python310\\site-packages (from portalocker<3,>=1.4->msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (306)\n",
      "Installing collected packages: llama-index-llms-azure-openai\n",
      "Successfully installed llama-index-llms-azure-openai-0.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-llms-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving subscriptions...\n",
      "Logged in to Azure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: your version of azd is out of date, you have 1.9.6 and the latest version is 1.10.1\n",
      "\n",
      "To update to the latest version, run:\n",
      "winget upgrade Microsoft.Azd\n"
     ]
    }
   ],
   "source": [
    "!azd auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from backend.auth.auth_utils import get_authenticated_user_details\n",
    "from backend.security.ms_defender_utils import get_msdefender_user_json\n",
    "from quart import request\n",
    "from llama_index.llms.azure_openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = os.environ.get(\"AZURE_OPENAI_PREVIEW_API_VERSION\")\n",
    "aoai_api_key = os.environ.get(\"AZURE_OPENAI_KEY\")\n",
    "ad_token_provider = get_bearer_token_provider(DefaultAzureCredential(),\"https://cognitiveservices.azure.com/.default\")\n",
    "endpoint = (\n",
    "            os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "            if os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "            else f\"https://{os.environ.get('AZURE_OPENAI_RESOURCE')}.openai.azure.com/\"\n",
    "        )\n",
    "default_headers = {\"x-ms-useragent\": \"GitHubSampleWebApp/AsyncAzureOpenAI/1.0.0\"}\n",
    "\n",
    "\n",
    "authenticated_user_details = get_authenticated_user_details({})\n",
    "conversation_id = None     \n",
    "user_json = get_msdefender_user_json(authenticated_user_details, {}, conversation_id)\n",
    "\n",
    "\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=os.environ.get(\"AZURE_OPENAI_MODEL\"),\n",
    "    model=os.environ.get(\"AZURE_OPENAI_MODEL_NAME\"),\n",
    "    api_key=aoai_api_key,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_version=api_version,\n",
    "    AZURE_AD_TOKEN_PROVIDER = ad_token_provider,\n",
    "    use_azure_ad =True,\n",
    "    kwargs={'default_headers': default_headers},\n",
    "    additional_kwargs={\"user\": user_json}\n",
    ")\n",
    "\n",
    "# llm = AzureChatOpenAI(\n",
    "#     deployment_name = os.environ.get(\"AZURE_OPENAI_MODEL\"),\n",
    "#     openai_api_version = api_version,\n",
    "#     openai_api_key = aoai_api_key,\n",
    "#     azure_ad_token_provider = ad_token_provider,\n",
    "#     default_headers = default_headers,\n",
    "#     azure_endpoint = endpoint,\n",
    "#     include_response_headers = True,\n",
    "#     # streaming= True,\n",
    "#     temperature= 0.15,\n",
    "#     # user_json = user_json\n",
    "#     model_kwargs = {'': user_json}\n",
    "#     # logprobs=True,\n",
    "#     # model_kwargs={\"stream_options\": {\"include_usage\": True}}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Ahoy there, matey! Welcome aboard! What brings ye to these treacherous waters today? Lookin' for treasure, a tale, or perhaps a bit o' both? Arrr!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with colorful personality.\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Hello\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-A5CXgovSVoyppRbqcfwwvosTHkzVt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Ahoy there, matey! Welcome aboard! What brings ye to these treacherous waters today? Lookin' for treasure, a tale, or perhaps a bit o' both? Arrr!\", refusal=None, role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1725803216, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_80a1bad4c7', usage=CompletionUsage(completion_tokens=41, prompt_tokens=20, total_tokens=61), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Completions.create() got an unexpected keyword argument 'user_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     (\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      7\u001b[0m ]\n\u001b[1;32m----> 8\u001b[0m ai_msg \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# apim_request_id = ai_msg.response_metadata['headers']['apim-request-id']\u001b[39;00m\n\u001b[0;32m     10\u001b[0m ai_msg\n",
      "File \u001b[1;32mc:\\Users\\v-samomin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:287\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    283\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    284\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    286\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 287\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    288\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    289\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    290\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    292\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    293\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    294\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    295\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    296\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    297\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\v-samomin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:787\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    781\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    785\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    786\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\v-samomin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:644\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    643\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    645\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    646\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    648\u001b[0m ]\n\u001b[0;32m    649\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\v-samomin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:634\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    633\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 634\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    635\u001b[0m                 m,\n\u001b[0;32m    636\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    637\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    638\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    639\u001b[0m             )\n\u001b[0;32m    640\u001b[0m         )\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\v-samomin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:856\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 856\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    857\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    858\u001b[0m         )\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    860\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\v-samomin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:638\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    636\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers:\n\u001b[1;32m--> 638\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m    639\u001b[0m     response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m    640\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[1;32mc:\\Users\\v-samomin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_legacy_response.py:350\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    348\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\v-samomin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: Completions.create() got an unexpected keyword argument 'user_json'"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that helps user.\",\n",
    "    ),\n",
    "    (\"human\", \"How are you?\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "# apim_request_id = ai_msg.response_metadata['headers']['apim-request-id']\n",
    "ai_msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'messages': [{'role': 'user', 'content': 'Registering a PDM Link Server'}],\n",
    " 'temperature': 0.0,\n",
    " 'max_tokens': 800,\n",
    " 'top_p': 1.0,\n",
    " 'stop': None,\n",
    " 'stream': True,\n",
    " 'model': 'ssagpt4o',\n",
    " 'user': '{\"EndUserId\": \"00000000-0000-0000-0000-000000000000\", \"EndUserIdType\": \"EntraId\", \"SourceIp\": \"127.0.0.1\", \"SourceRequestHeaders\": {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"}, \"ConversationId\": null}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object BaseChatModel.ainvoke at 0x000002C49B9065E0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.utils import (\n",
    "    format_as_ndjson,\n",
    "    format_stream_response,\n",
    "    format_non_streaming_response,\n",
    "    convert_to_pf_format,\n",
    "    format_pf_non_streaming_response,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.include_response_headers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content='' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=\"I'm\" additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' just' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' a' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' computer' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' program' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' so' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' I' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=\" don't\" additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' have' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' feelings' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' but' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=\" I'm\" additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' here' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' and' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' ready' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' to' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' help' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' How' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' can' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' I' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' assist' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content=' today' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content='?' additional_kwargs={} response_metadata={} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_80a1bad4c7'} id='run-0f7394fb-863d-4e95-aa29-ff61bbbe0094'\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(messages):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finish_reason': 'stop',\n",
       " 'model_name': 'gpt-4o-2024-05-13',\n",
       " 'system_fingerprint': 'fp_80a1bad4c7'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
